{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Direct Training - No Curriculum\n",
        "\n",
        "Train directly on target environment without curriculum learning.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, warnings\n",
        "os.environ[\"PYTHONWARNINGS\"] = \"ignore\"\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import time\n",
        "import torch\n",
        "\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.callbacks import BaseCallback\n",
        "\n",
        "from src.environment import make_vec_env\n",
        "from src.evaluation import evaluate\n",
        "from src.cnn import get_policy_kwargs\n",
        "from src.filemanager import FileManager"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment directory initialized: results/direct_training_20251028_181546\n"
          ]
        }
      ],
      "source": [
        "fm: FileManager = FileManager(\"direct_training\", output_dir=\"results\")      "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation Callback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class StepCallback(BaseCallback):\n",
        "    \"\"\"Periodic evaluation\"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        env_name: str,\n",
        "        eval_freq: int = 5_000,\n",
        "        n_eval: int = 30,\n",
        "        visualize: bool = True,\n",
        "        checkpoint_freq: int = 100_000\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.env_name: str = env_name\n",
        "        self.eval_freq: int = eval_freq\n",
        "        self.n_eval: int = n_eval\n",
        "        self.visualize: bool = visualize\n",
        "        self.checkpoint_freq: int = checkpoint_freq\n",
        "        self.steps: int = 0\n",
        "        self.start_time: float = time.time()\n",
        "        self.total_training_time: float = 0.0\n",
        "    \n",
        "    def _on_step(self) -> bool:\n",
        "        self.steps += 1\n",
        "        \n",
        "        if self.steps % self.eval_freq == 0:\n",
        "            assert isinstance(self.model, PPO)\n",
        "            \n",
        "            # Evaluate\n",
        "            episode_batch = evaluate(self.model, self.env_name, self.n_eval)\n",
        "            \n",
        "            # Calculate elapsed time for this stage\n",
        "            self.total_training_time = time.time() - self.start_time\n",
        "            \n",
        "            # Print evaluation results\n",
        "            print(\n",
        "                f\"  Eval @ Total: {self.steps:,} | \"\n",
        "                f\"Success: {episode_batch.success_rate:.1%} | \"\n",
        "                f\"Len: {episode_batch.mean_length:.1f} | \"\n",
        "                f\"Reward: {episode_batch.mean_reward:.2f} | \"\n",
        "                f\"PolicyEnt: {episode_batch.mean_entropy:.3f} | \"\n",
        "                f\"Time: {int(self.total_training_time//60):02d}:{int(self.total_training_time%60):02d}\"\n",
        "            )\n",
        "            \n",
        "            # Write evaluation\n",
        "            fm.dump_eval_to_csv(\n",
        "                total_step=self.steps,\n",
        "                stage=\"direct\",\n",
        "                stage_step=self.steps,\n",
        "                batch=episode_batch,\n",
        "                model=self.model,\n",
        "                allocation={}\n",
        "            )\n",
        "            \n",
        "            # Visualize\n",
        "            if self.visualize:\n",
        "                from src.episode_visualization import visualize_eval_episode\n",
        "                visualize_eval_episode(\n",
        "                    model=self.model,\n",
        "                    episode=episode_batch.episodes[0],\n",
        "                    timestep=self.steps,\n",
        "                    output_dir=fm.get_visualization_dir()\n",
        "                )\n",
        "        \n",
        "        if self.steps % self.checkpoint_freq == 0:\n",
        "            fm.save_checkpoint(self.model, \"direct\", self.steps)\n",
        "\n",
        "        return True\n",
        "\n",
        "    def get_total_time(self) -> float:\n",
        "        \"\"\"Get total training time.\"\"\"\n",
        "        return self.total_training_time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "N_ENVS: int = 8\n",
        "N_STEPS: int = 128\n",
        "STEPS_PER_ROLLOUT = N_STEPS * N_ENVS\n",
        "\n",
        "# Evaluate every 5 rollout\n",
        "EVAL_FREQ: int = 5 * STEPS_PER_ROLLOUT\n",
        "N_EVALS: int = 100\n",
        "\n",
        "device = \"\"\n",
        "if torch.cuda.is_available(): # type: ignore\n",
        "    device = \"cuda\"\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# https://minigrid.farama.org/environments/minigrid/\n",
        "ENV_NAME = \"MiniGrid-DoorKey-8x8-v0\"\n",
        "\n",
        "TOTAL_STEPS: int = 70_000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create env and model\n",
        "env = make_vec_env(ENV_NAME, N_ENVS)\n",
        "model: PPO = PPO(\n",
        "        \"CnnPolicy\",\n",
        "        env,\n",
        "        policy_kwargs=get_policy_kwargs(),\n",
        "        learning_rate= 3e-4,\n",
        "        n_steps=N_STEPS,\n",
        "        batch_size=64,\n",
        "        n_epochs=10,\n",
        "        gamma=0.99,\n",
        "        gae_lambda=0.95,\n",
        "        clip_range=0.2,\n",
        "        ent_coef=0.02,\n",
        "        verbose=0,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "# Callback with tracking\n",
        "callback = StepCallback(\n",
        "    ENV_NAME,\n",
        "    EVAL_FREQ,\n",
        "    n_eval=N_EVALS,\n",
        "    visualize=True,\n",
        "    checkpoint_freq=100_000\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Direct Training: MiniGrid-DoorKey-8x8-v0\n",
            "============================================================\n",
            "  Eval @ Total: 5,120 | Success: 3.0% | Len: 634.0 | Reward: 0.01 | PolicyEnt: 1.774 | Time: 04:21\n",
            "    → Evaluation saved to: results/direct_training_20251028_181546/evaluations.csv\n",
            "    → Saved visualization: results/direct_training_20251028_181546/visualizations/eval_5120.png\n",
            "  Eval @ Total: 10,240 | Success: 0.0% | Len: 640.0 | Reward: 0.00 | PolicyEnt: 1.657 | Time: 08:11\n",
            "    → Evaluation saved to: results/direct_training_20251028_181546/evaluations.csv\n",
            "    → Saved visualization: results/direct_training_20251028_181546/visualizations/eval_10240.png\n",
            "  Eval @ Total: 15,360 | Success: 0.0% | Len: 640.0 | Reward: 0.00 | PolicyEnt: 1.706 | Time: 12:36\n",
            "    → Evaluation saved to: results/direct_training_20251028_181546/evaluations.csv\n",
            "    → Saved visualization: results/direct_training_20251028_181546/visualizations/eval_15360.png\n",
            "  Eval @ Total: 20,480 | Success: 0.0% | Len: 640.0 | Reward: 0.00 | PolicyEnt: 1.747 | Time: 16:37\n",
            "    → Evaluation saved to: results/direct_training_20251028_181546/evaluations.csv\n",
            "    → Saved visualization: results/direct_training_20251028_181546/visualizations/eval_20480.png\n",
            "  Eval @ Total: 25,600 | Success: 0.0% | Len: 640.0 | Reward: 0.00 | PolicyEnt: 1.673 | Time: 21:28\n",
            "    → Evaluation saved to: results/direct_training_20251028_181546/evaluations.csv\n",
            "    → Saved visualization: results/direct_training_20251028_181546/visualizations/eval_25600.png\n",
            "  Eval @ Total: 30,720 | Success: 1.0% | Len: 637.1 | Reward: 0.01 | PolicyEnt: 1.614 | Time: 25:51\n",
            "    → Evaluation saved to: results/direct_training_20251028_181546/evaluations.csv\n",
            "    → Saved visualization: results/direct_training_20251028_181546/visualizations/eval_30720.png\n",
            "  Eval @ Total: 35,840 | Success: 0.0% | Len: 640.0 | Reward: 0.00 | PolicyEnt: 1.572 | Time: 30:00\n",
            "    → Evaluation saved to: results/direct_training_20251028_181546/evaluations.csv\n",
            "    → Saved visualization: results/direct_training_20251028_181546/visualizations/eval_35840.png\n",
            "  Eval @ Total: 40,960 | Success: 0.0% | Len: 640.0 | Reward: 0.00 | PolicyEnt: 1.700 | Time: 34:20\n",
            "    → Evaluation saved to: results/direct_training_20251028_181546/evaluations.csv\n",
            "    → Saved visualization: results/direct_training_20251028_181546/visualizations/eval_40960.png\n",
            "  Eval @ Total: 46,080 | Success: 0.0% | Len: 640.0 | Reward: 0.00 | PolicyEnt: 1.633 | Time: 38:26\n",
            "    → Evaluation saved to: results/direct_training_20251028_181546/evaluations.csv\n",
            "    → Saved visualization: results/direct_training_20251028_181546/visualizations/eval_46080.png\n",
            "  Eval @ Total: 51,200 | Success: 1.0% | Len: 639.7 | Reward: 0.00 | PolicyEnt: 1.550 | Time: 42:56\n",
            "    → Evaluation saved to: results/direct_training_20251028_181546/evaluations.csv\n",
            "    → Saved visualization: results/direct_training_20251028_181546/visualizations/eval_51200.png\n",
            "  Eval @ Total: 56,320 | Success: 0.0% | Len: 640.0 | Reward: 0.00 | PolicyEnt: 1.657 | Time: 47:01\n",
            "    → Evaluation saved to: results/direct_training_20251028_181546/evaluations.csv\n",
            "    → Saved visualization: results/direct_training_20251028_181546/visualizations/eval_56320.png\n",
            "  Eval @ Total: 61,440 | Success: 0.0% | Len: 640.0 | Reward: 0.00 | PolicyEnt: 1.613 | Time: 51:19\n",
            "    → Evaluation saved to: results/direct_training_20251028_181546/evaluations.csv\n",
            "    → Saved visualization: results/direct_training_20251028_181546/visualizations/eval_61440.png\n",
            "  Eval @ Total: 66,560 | Success: 2.0% | Len: 633.5 | Reward: 0.01 | PolicyEnt: 1.649 | Time: 56:27\n",
            "    → Evaluation saved to: results/direct_training_20251028_181546/evaluations.csv\n",
            "    → Saved visualization: results/direct_training_20251028_181546/visualizations/eval_66560.png\n",
            "    → Checkpoint saved: results/direct_training_20251028_181546/checkpoints/direct_step_70400.zip\n",
            "\n",
            "============================================================\n",
            "DIRECT TRAINING COMPLETE\n",
            "============================================================\n",
            "Total training time: 56:27\n",
            "Total steps: 70,400\n"
          ]
        }
      ],
      "source": [
        "# Train\n",
        "print(f\"\\n{'='*60}\\nDirect Training: {ENV_NAME}\\n{'='*60}\")\n",
        "\n",
        "while callback.steps < TOTAL_STEPS:\n",
        "    model.learn( # type: ignore\n",
        "        total_timesteps=EVAL_FREQ,\n",
        "        callback=callback,\n",
        "        reset_num_timesteps=False\n",
        "    )\n",
        "\n",
        "env.close()\n",
        "\n",
        "# Save final checkpoint\n",
        "fm.save_checkpoint(model, \"direct\", callback.steps)\n",
        "\n",
        "total_time: float = callback.get_total_time()\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"DIRECT TRAINING COMPLETE\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"Total training time: {int(total_time//60):02d}:{int(total_time%60):02d}\")\n",
        "print(f\"Total steps: {callback.steps:,}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
