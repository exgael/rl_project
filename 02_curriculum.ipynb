{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Curriculum Learning - Self-Contained\n",
        "\n",
        "Automatic curriculum learning for MiniGrid environments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, warnings\n",
        "os.environ[\"PYTHONWARNINGS\"] = \"ignore\"\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from typing import List, Dict\n",
        "import numpy as np\n",
        "import time\n",
        "import torch\n",
        "\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.callbacks import BaseCallback\n",
        "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
        "\n",
        "from src.environment import make_vec_env\n",
        "from src.evaluation import evaluate\n",
        "from src.cnn import get_policy_kwargs\n",
        "from src.filemanager import FileManager"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Result directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment directory initialized: results/curriculum_learning_20251028_164453\n"
          ]
        }
      ],
      "source": [
        "fm: FileManager = FileManager(\"curriculum_learning\", output_dir=\"results\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Curriculum Teacher\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CurriculumTeacher:\n",
        "    \"\"\"Manages automatic progression through stages.\"\"\"\n",
        "    \n",
        "    def __init__(self, stages: List[str], threshold: float = 0.90, window: int = 5) -> None:\n",
        "        self.stages: List[str] = stages\n",
        "        self.threshold: float = threshold\n",
        "        self.window: int = window\n",
        "        self.stage_idx: int = 0\n",
        "        self.performance: Dict[str, List[float]] = {s: [] for s in stages}\n",
        "        self._completed: bool = False\n",
        "    \n",
        "    def record(self, success_rate: float) -> None:\n",
        "        \"\"\"Record performance for current stage.\"\"\"\n",
        "        self.performance[self.current_stage()].append(success_rate)\n",
        "    \n",
        "    def should_advance(self) -> bool:\n",
        "        \"\"\"No eval below threshold.\"\"\"\n",
        "        history = self.performance[self.current_stage()]\n",
        "        \n",
        "        if len(history) < self.window:\n",
        "            return False\n",
        "        \n",
        "        recent = history[-self.window:]\n",
        "        min_recent = float(np.min(recent))\n",
        "        \n",
        "        return min_recent >= self.threshold\n",
        "\n",
        "    \n",
        "    def advance(self) -> None:\n",
        "        \"\"\"Move to next stage or mark as complete.\"\"\"\n",
        "        if self.stage_idx < len(self.stages) - 1:\n",
        "            self.stage_idx += 1\n",
        "        else:\n",
        "            self._completed = True\n",
        "    \n",
        "    def current_stage(self) -> str:\n",
        "        \"\"\"Get current stage name.\"\"\"\n",
        "        return self.stages[self.stage_idx]\n",
        "    \n",
        "    def is_all_stages_complete(self) -> bool:\n",
        "        \"\"\"Check if all stages completed.\"\"\"\n",
        "        return self._completed\n",
        "    \n",
        "    def get_current_performance_summary(self) -> str:\n",
        "        \"\"\"Get human-readable summary of current stage performance.\"\"\"\n",
        "        history = self.performance[self.current_stage()]\n",
        "        if not history:\n",
        "            return \"No evaluations yet\"\n",
        "        \n",
        "        recent = history[-self.window:] if len(history) >= self.window else history\n",
        "        mean = np.mean(recent)\n",
        "        summary = f\"Last {len(recent)}: {mean:.1%}\"\n",
        "       \n",
        "        return summary\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step Callback\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "class StepCallback(BaseCallback):\n",
        "    \"\"\"Periodic evaluation and entropy management.\"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        teacher: CurriculumTeacher,\n",
        "        eval_freq: int = 5_000,\n",
        "        n_eval: int = 30,\n",
        "        visualize: bool = True,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.teacher: CurriculumTeacher = teacher\n",
        "        self.eval_freq: int = eval_freq\n",
        "        self.n_eval: int = n_eval\n",
        "        self.visualize: bool = visualize\n",
        "        self.stage_steps: int = 0\n",
        "        self.total_steps: int = 0\n",
        "        self.stage_start_time: float = time.time()\n",
        "        self.total_training_time: float = 0.0\n",
        "    \n",
        "    def _on_step(self) -> bool:\n",
        "        self.stage_steps += 1\n",
        "        self.total_steps += 1\n",
        "        \n",
        "        if self.stage_steps % self.eval_freq == 0:\n",
        "            assert isinstance(self.model, PPO)\n",
        "            \n",
        "            # Evaluate\n",
        "            episode_batch = evaluate(self.model, self.teacher.current_stage(), self.n_eval)\n",
        "            self.teacher.record(episode_batch.success_rate)\n",
        "            \n",
        "            # Calculate elapsed times\n",
        "            stage_elapsed: float = time.time() - self.stage_start_time\n",
        "            total_elapsed: float = self.get_total_time()\n",
        "            \n",
        "            # Print evaluation results\n",
        "            print(\n",
        "                f\"  Eval @ Stage {self.stage_steps:,} | Total: {self.total_steps:,} | \"\n",
        "                f\"Success: {episode_batch.success_rate:.1%} | \"\n",
        "                f\"Reward: {episode_batch.mean_reward:.2f} | \"\n",
        "                f\"PolicyEnt: {episode_batch.mean_entropy:.3f} | \"\n",
        "                f\"StageTime: {int(stage_elapsed//60):02d}:{int(stage_elapsed%60):02d} | \"\n",
        "                f\"TotalTime: {int(total_elapsed//60):02d}:{int(total_elapsed%60):02d}\"\n",
        "            )\n",
        "            \n",
        "            # Write evaluation\n",
        "            fm.dump_eval_to_csv(\n",
        "                total_step=self.total_steps,\n",
        "                stage=self.teacher.current_stage(),\n",
        "                stage_step=self.stage_steps,\n",
        "                batch=episode_batch,\n",
        "                model=self.model\n",
        "            )\n",
        "            \n",
        "            # Visualize\n",
        "            if self.visualize:\n",
        "                from src.episode_visualization import visualize_eval_episode\n",
        "                visualize_eval_episode(\n",
        "                    model=self.model,\n",
        "                    episode=episode_batch.episodes[0],\n",
        "                    timestep=self.total_steps,\n",
        "                    output_dir=fm.get_visualization_dir()\n",
        "                )\n",
        "        \n",
        "        return True\n",
        "\n",
        "    \n",
        "    def reset_for_stage(self) -> None:\n",
        "        \"\"\"Reset stage counter and timer for new environment.\"\"\"\n",
        "        # Accumulate time from completed stage\n",
        "        stage_elapsed: float = time.time() - self.stage_start_time\n",
        "        self.total_training_time += stage_elapsed\n",
        "        \n",
        "        # Reset for new stage\n",
        "        self.stage_steps = 0\n",
        "        self.stage_start_time = time.time()\n",
        "    \n",
        "    def get_stage_elapsed(self) -> float:\n",
        "        \"\"\"Get elapsed time for current stage in seconds.\"\"\"\n",
        "        return time.time() - self.stage_start_time\n",
        "    \n",
        "    def get_total_time(self) -> float:\n",
        "        \"\"\"Get total training time across all stages in seconds.\"\"\"\n",
        "        return self.total_training_time + self.get_stage_elapsed()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "N_ENVS: int = 8\n",
        "N_STEPS: int = 128\n",
        "STEPS_PER_ROLLOUT = N_STEPS * N_ENVS\n",
        "\n",
        "# Evaluate every 5 rollout\n",
        "EVAL_FREQ: int = 5 * STEPS_PER_ROLLOUT\n",
        "N_EVALS: int = 100\n",
        "\n",
        "device = \"\"\n",
        "if torch.cuda.is_available(): # type: ignore\n",
        "    device = \"cuda\"\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Curriculum\n",
        "THRESHOLD: float = 0.90\n",
        "WINDOW: int = 4\n",
        "\n",
        "# https://minigrid.farama.org/environments/minigrid/\n",
        "\n",
        "STAGES: List[str] = [\n",
        "    \"MiniGrid-DoorKey-5x5-v0\",\n",
        "    \"MiniGrid-DoorKey-6x6-v0\",\n",
        "    \"MiniGrid-DoorKey-8x8-v0\",\n",
        "]\n",
        "\n",
        "\n",
        "TOTAL_STEPS: int = 200_000\n",
        "\n",
        "# PPO\n",
        "def make_model(env: SubprocVecEnv) -> PPO:\n",
        "    return PPO(\n",
        "        \"CnnPolicy\",\n",
        "        env,\n",
        "        policy_kwargs=get_policy_kwargs(),\n",
        "        learning_rate= 3e-4,\n",
        "        n_steps=N_STEPS,\n",
        "        batch_size=64,\n",
        "        n_epochs=10,\n",
        "        gamma=0.99,\n",
        "        gae_lambda=0.95,\n",
        "        clip_range=0.2,\n",
        "        ent_coef=0.02,\n",
        "        verbose=0,\n",
        "        device=device\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_curriculum() -> None:\n",
        "\n",
        "    # Teacher\n",
        "    teacher: CurriculumTeacher = CurriculumTeacher(STAGES, threshold=THRESHOLD, window=WINDOW)\n",
        "\n",
        "    # Initial env and model\n",
        "    env = make_vec_env(teacher.current_stage(), N_ENVS)\n",
        "    model = make_model(env)\n",
        "\n",
        "    # Callback with tracking\n",
        "    callback: StepCallback = StepCallback(\n",
        "        teacher,\n",
        "        eval_freq=EVAL_FREQ,\n",
        "        n_eval=N_EVALS,\n",
        "        visualize=True,\n",
        "    )\n",
        "\n",
        "    # Train through stages\n",
        "    while not teacher.is_all_stages_complete():\n",
        "        print(f\"\\n{'='*60}\\nStage {teacher.stage_idx + 1}/{len(STAGES)}: {teacher.current_stage()}\\n{'='*60}\")\n",
        "        # Train until stage mastered or max steps\n",
        "        while callback.total_steps < TOTAL_STEPS:\n",
        "            model.learn(  # type: ignore\n",
        "                total_timesteps=EVAL_FREQ,\n",
        "                callback=callback,\n",
        "                reset_num_timesteps=False\n",
        "            )\n",
        "            \n",
        "            # Check advancement\n",
        "            if teacher.should_advance():\n",
        "                print(f\"\\n  ✓ Stage passed after {callback.stage_steps:,} steps\")\n",
        "                # Save checkpoint for this completed stage\n",
        "                fm.save_checkpoint(\n",
        "                    model=model,\n",
        "                    stage=teacher.current_stage(),\n",
        "                    total_step=callback.total_steps\n",
        "                )\n",
        "                break\n",
        "\n",
        "        if callback.total_steps >= TOTAL_STEPS:\n",
        "            raise ValueError(f\"Training failed after {callback.total_steps:,} steps\")\n",
        "\n",
        "        # Advance\n",
        "        teacher.advance()\n",
        "        if not teacher.is_all_stages_complete():\n",
        "            env.close()\n",
        "            callback.reset_for_stage()\n",
        "            env = make_vec_env(teacher.current_stage(), N_ENVS)\n",
        "            model.set_env(env)  # type: ignore\n",
        "    \n",
        "    env.close()\n",
        "    total_time = callback.get_total_time()\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"CURRICULUM COMPLETE\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"Total training time: {int(total_time//60):02d}:{int(total_time%60):02d}\")\n",
        "    print(f\"Total steps: {callback.total_steps:,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Stage 1/2: MiniGrid-Unlock-v0\n",
            "============================================================\n",
            "  Eval @ Stage 5,120 | Total: 5,120 | Success: 1.0% | Reward: 0.00 | PolicyEnt: 1.717 | StageTime: 02:29 | TotalTime: 02:29\n",
            "    → Evaluation saved to: results/curriculum_learning_20251028_164453/evaluations.csv\n",
            "    → Saved visualization: results/curriculum_learning_20251028_164453/visualizations/eval_5120.png\n",
            "  Eval @ Stage 10,240 | Total: 10,240 | Success: 2.0% | Reward: 0.01 | PolicyEnt: 1.783 | StageTime: 04:55 | TotalTime: 04:55\n",
            "    → Evaluation saved to: results/curriculum_learning_20251028_164453/evaluations.csv\n",
            "    → Saved visualization: results/curriculum_learning_20251028_164453/visualizations/eval_10240.png\n",
            "  Eval @ Stage 15,360 | Total: 15,360 | Success: 13.0% | Reward: 0.06 | PolicyEnt: 1.670 | StageTime: 07:17 | TotalTime: 07:17\n",
            "    → Evaluation saved to: results/curriculum_learning_20251028_164453/evaluations.csv\n",
            "    → Saved visualization: results/curriculum_learning_20251028_164453/visualizations/eval_15360.png\n",
            "  Eval @ Stage 20,480 | Total: 20,480 | Success: 48.0% | Reward: 0.27 | PolicyEnt: 1.525 | StageTime: 09:27 | TotalTime: 09:27\n",
            "    → Evaluation saved to: results/curriculum_learning_20251028_164453/evaluations.csv\n",
            "    → Saved visualization: results/curriculum_learning_20251028_164453/visualizations/eval_20480.png\n",
            "  Eval @ Stage 25,600 | Total: 25,600 | Success: 55.0% | Reward: 0.35 | PolicyEnt: 1.267 | StageTime: 11:28 | TotalTime: 11:28\n",
            "    → Evaluation saved to: results/curriculum_learning_20251028_164453/evaluations.csv\n",
            "    → Saved visualization: results/curriculum_learning_20251028_164453/visualizations/eval_25600.png\n",
            "  Eval @ Stage 30,720 | Total: 30,720 | Success: 93.0% | Reward: 0.70 | PolicyEnt: 1.212 | StageTime: 12:57 | TotalTime: 12:57\n",
            "    → Evaluation saved to: results/curriculum_learning_20251028_164453/evaluations.csv\n",
            "    → Saved visualization: results/curriculum_learning_20251028_164453/visualizations/eval_30720.png\n",
            "  Eval @ Stage 35,840 | Total: 35,840 | Success: 99.0% | Reward: 0.86 | PolicyEnt: 0.921 | StageTime: 14:11 | TotalTime: 14:11\n",
            "    → Evaluation saved to: results/curriculum_learning_20251028_164453/evaluations.csv\n",
            "    → Saved visualization: results/curriculum_learning_20251028_164453/visualizations/eval_35840.png\n",
            "  Eval @ Stage 40,960 | Total: 40,960 | Success: 100.0% | Reward: 0.89 | PolicyEnt: 0.733 | StageTime: 15:19 | TotalTime: 15:19\n",
            "    → Evaluation saved to: results/curriculum_learning_20251028_164453/evaluations.csv\n",
            "    → Saved visualization: results/curriculum_learning_20251028_164453/visualizations/eval_40960.png\n",
            "  Eval @ Stage 46,080 | Total: 46,080 | Success: 100.0% | Reward: 0.91 | PolicyEnt: 0.789 | StageTime: 16:29 | TotalTime: 16:29\n",
            "    → Evaluation saved to: results/curriculum_learning_20251028_164453/evaluations.csv\n",
            "    → Saved visualization: results/curriculum_learning_20251028_164453/visualizations/eval_46080.png\n",
            "\n",
            "  ✓ Stage passed after 46,080 steps\n",
            "    → Checkpoint saved: results/curriculum_learning_20251028_164453/checkpoints/MiniGrid-Unlock-v0_step_46080.zip\n",
            "\n",
            "============================================================\n",
            "Stage 2/2: MiniGrid-UnlockPickup-v0\n",
            "============================================================\n",
            "  Eval @ Stage 5,120 | Total: 51,200 | Success: 1.0% | Reward: 0.00 | PolicyEnt: 1.575 | StageTime: 02:34 | TotalTime: 19:06\n",
            "    → Evaluation saved to: results/curriculum_learning_20251028_164453/evaluations.csv\n",
            "    → Saved visualization: results/curriculum_learning_20251028_164453/visualizations/eval_51200.png\n",
            "  Eval @ Stage 10,240 | Total: 56,320 | Success: 1.0% | Reward: 0.00 | PolicyEnt: 1.613 | StageTime: 05:14 | TotalTime: 21:46\n",
            "    → Evaluation saved to: results/curriculum_learning_20251028_164453/evaluations.csv\n",
            "    → Saved visualization: results/curriculum_learning_20251028_164453/visualizations/eval_56320.png\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrain_curriculum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36mtrain_curriculum\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Train until stage mastered or max steps\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m callback.total_steps < TOTAL_STEPS:\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m     \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[32m     24\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mEVAL_FREQ\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m     \u001b[38;5;66;03m# Check advancement\u001b[39;00m\n\u001b[32m     30\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m teacher.should_advance():\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/dev/rl_curriculum_learning/.venv/lib/python3.12/site-packages/stable_baselines3/ppo/ppo.py:311\u001b[39m, in \u001b[36mPPO.learn\u001b[39m\u001b[34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[39m\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlearn\u001b[39m(\n\u001b[32m    303\u001b[39m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[32m    304\u001b[39m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    309\u001b[39m     progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    310\u001b[39m ) -> SelfPPO:\n\u001b[32m--> \u001b[39m\u001b[32m311\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    313\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/dev/rl_curriculum_learning/.venv/lib/python3.12/site-packages/stable_baselines3/common/on_policy_algorithm.py:337\u001b[39m, in \u001b[36mOnPolicyAlgorithm.learn\u001b[39m\u001b[34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[39m\n\u001b[32m    334\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ep_info_buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    335\u001b[39m         \u001b[38;5;28mself\u001b[39m.dump_logs(iteration)\n\u001b[32m--> \u001b[39m\u001b[32m337\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    339\u001b[39m callback.on_training_end()\n\u001b[32m    341\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/dev/rl_curriculum_learning/.venv/lib/python3.12/site-packages/stable_baselines3/ppo/ppo.py:275\u001b[39m, in \u001b[36mPPO.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    273\u001b[39m \u001b[38;5;66;03m# Optimization step\u001b[39;00m\n\u001b[32m    274\u001b[39m \u001b[38;5;28mself\u001b[39m.policy.optimizer.zero_grad()\n\u001b[32m--> \u001b[39m\u001b[32m275\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    276\u001b[39m \u001b[38;5;66;03m# Clip grad norm\u001b[39;00m\n\u001b[32m    277\u001b[39m th.nn.utils.clip_grad_norm_(\u001b[38;5;28mself\u001b[39m.policy.parameters(), \u001b[38;5;28mself\u001b[39m.max_grad_norm)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/dev/rl_curriculum_learning/.venv/lib/python3.12/site-packages/torch/_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/dev/rl_curriculum_learning/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/dev/rl_curriculum_learning/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "train_curriculum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from src.environment import run_episode\n",
        "# model_path = \"results/curriculum_learning_20251027_134503/checkpoints/MiniGrid-KeyCorridorS3R2-v0_step_475136.zip\"\n",
        "# model = PPO.load(model_path) # type: ignore\n",
        "# episode_data = run_episode(\n",
        "#     model=model, \n",
        "#     env_name=\"MiniGrid-KeyCorridorS3R2-v0\", \n",
        "#     seed=42, \n",
        "#     render_mode=\"human\", \n",
        "#     deterministic=True\n",
        "# )\n",
        "\n",
        "# print(episode_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from src.episode_visualization import visualize_eval_episode\n",
        "# visualize_eval_episode(\n",
        "#     model=model,\n",
        "#     episode=episode_data,\n",
        "#     timestep=-1,\n",
        "#     output_dir=\"./\"\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
